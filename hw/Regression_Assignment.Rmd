---
title: "Regression Assignment"
author: "Robin Donatello"
date: "Last Updated `r Sys.time()`"
output:
  html_document:
    highlight: tango
    theme: flatly
    toc: yes
    toc_float: yes
  pdf_document: default
css: ../css/customh5.css
---

# Assignment Overview

You will perform <span style="color:blue">3</span> regression analyses in this assignment. For each you will interpret the regression coefficients, and test for a potential confounder.

1. Multiple Linear Regression: $Q \sim Q + B$
    - The bivariate $Q \sim Q$ relationship must be significant. 
2. Logistic Regression: $B \sim Q + B$
3. Log-Linear Regression: $log(Q) \sim Q + B$

You will then take one of the three above analyses and 

4. Add a third categorical (more than 2 levels) variable e.g.: $Q \sim Q + C$.
5. Conduct a Likelihood Ratio Test to compare the model fit with and without the additional categorical variable. 


#### Submission Guidelines

* Use the template provided: [[RMD]]() for R users, and [[Word]]() for SPSS users. 
* Upload to the corresponding assignment on Blackboard by the due date. 
* Share and learn from others by post a copy of this assignment to your blog as you complete each stage. 


# Instructions
1. Identify variables under consideration
    - Determine a third variable that you want to test as a potential confounder.  
    - Consider the relationship and ask yourself: "_If I had to predict a future persons response based on the Predictor/Explanatory variable and some other variable(s), what would they be?_"
    - For the purposes of this assignment, your <span style="color:red">potential confounder must be binary</span>.
    - For the purposes of this assignment, your <span style="color:red"> Q~Q relationship must be significant</span>. This is only required for the multivariable linear regression model. 
2. Write out the null, alternative, and confounder Hypotheses statements.
    - **Null** - that there is no relationship between response and explanatory variables
    - **Alternative** - that there is a relationship between response and explanatory variables. 
    - **Confounder** - that there is a relationship between response and explanatory variables after controlling for the confounding variable. 
3. Fit the simple model
    - Model the response variable on the explanatory variable `y ~ x`
    - Determine if you are going to reject the null in favor of the alternative.
4. Fit the multivariable model. 
    - Only do this if you rejected the null.
    - Model the response variable on the explanatory variable and the third variable. `y ~ x + z`
5. Interpret the Confounder
    - When the third variable is included in the code, look at the p-value for the explanatory variable. 
        - If it is still significant, the third variable is not a confounding variable. 
        - If it is no longer significant, the third variable is a confounding variable. This means that the third variable is explaining the relationship between the explanatory variable and the response variable.
6. Conclusion
    - Details included in each section below. 
    - A structured response template is provided. 

```{r, warning=FALSE, message=FALSE}
library(knitr)
opts_chunk$set(warning=FALSE, message=FALSE) 
library(dplyr)
library(ggplot2); library(gridExtra)
library(pander) # Used for printing nice linear model tables
panderOptions("digits", 3)
load("E:/MATH315/Project/addhealth_clean.Rdata")
```

# Multiple Linear Regression

## 1. Identify variables

If you have a “Strongly Agree” to “Strongly Disagree” variable that you have kept all 5 levels, you can treat it as a Quantitative Variable.

* Quantitative outcome: Income (variable `income`). 
* Quantitative predictor: Time you wake up in the morning (variable `wakeup`)
* Binary confounder: Gender (variable `female_c`)
 

## 2. State the research Hypothesis

* Null: There is no relationship between the time you wake up and your personal earnings
* Alternative: There is a relationship between the time you wake up and your personal earnings
* Confounder:  There is still a relationship between the time you wake up and your personal earnings, after controlling for gender. 


## 3. Fit the simple model

```{r}
lm.mod1 <- lm(income ~ wakeup, data=addhealth) # fit the model on wakeup only
pander(summary(lm.mod1), caption = "Modeling income using wakeup time.")
```

The p-value for the $b_{1}$ estimate of the regression coefficient for `wakeup` is significant at 0.001. There is reason to believe that the time you wakeup is associated with your income. 


## 4. Fit the multivariable model

Fit the same multiple linear regression model and include the potential confounding variable. Determine if the third variable is a confounder. 

```{r}
lm.mod2 <- lm(income ~ wakeup + female_c, data=addhealth) 
pander(summary(lm.mod2), caption="Modeling income using gender and wakeup time.")
```


## 5. Interpret the confounder

The relationship between income and wake up time is still significant after controlling for gender. Gender is not a confounder. 

## 6. Conclusion

* Use the numerical results from the multivariabel model to fill in the values in the conclusion below. 
* Look at the Adjusted $R^{2}$ to see how much of the variance in the response you are accounting for with the predictor.
* To get the confidence intervals for the regression coefficients you will use the function `confint()`. `kable` is used here to make a nicely formatted table upon knitting. 

```{r}
kable(confint(lm.mod2), digits=2)
```

Replace the **bold** words with your variables, the `highlighted` words with data from your analysis, and choose between _conclusion options_. 


After adjusting for the potential confounding factor of **third variable**, **explanatory variable** (b1 = `parameter estimate, CI confidence interval range, p = significance value`) was ***significantly/not significantly** and **positively/negatively** associated with **response variable**. Approximately `R-Square*100` of the variance of **response** can be accounted for by **explanatory** after controlling for **third variable**. Based on these analyses, **third variable** _is not/is_ a confounding factor because the association between **explanatory** and **response** _is still/is no longer_ significant after accounting for **third variable**. 

So the conclusion for this analysis reads: 

> After adjusting for the potential confounding factor of **gender**, **wake up time** ($\beta_{1}$ = `-611, -904, -318, p<.0001`) was **significantly** and **negatively** associated with **income**. Approximately `3.2%` of the variance of **income** can be accounted for by **wake up time** after controlling for **gender**. Based on these analyses, **gender** _is not_ a confounding factor because the association between **wake up time** and **income** _is still_ significant after accounting for **gender**.

Additional example interpretation

> After adjusting for the potential confounding factor of gender, an adolescent’s weight (Beta = 1.34, 95% CI -0.53, 3.21, p = .1558) was not significantly associated with the number of cigarettes smoked in the past 30 days. Approximately 0.78% of the variance in cigarettes smoked can be accounted for by weight after controlling for gender. Based on these analyses, gender is a confounding factor because the association between weight and cigarettes smoked is no longer significant after accounting for gender. 


# Logistic Regression

Your outcome variable must be coded as 1 (event) and 0 (non-event). Recoding this way ensures you are predicting the presence of your categorical variable and not the absence of it. 

## 1. Identify variables

* Binary outcome: Poverty (variable `poverty`). _To see how this was created check my [Add Health data management sample](https://norcalbiostat.github.io/MATH315/hw/dm_addhealth.html) page. 
* Quantitative predictor: Time you wake up in the morning (variable `wakeup`)
* Binary confounder: Gender (variable `female_c`)
 

## 2. State hypotheses

* Null hypothesis: There is no relationship between the probability of living below the poverty level and the time you wake up in the morning. 
* Alternative hypothesis: There is a relationship between the probability of living below the poverty level and the time you wake up in the morning. 
* Confounding hypothesis: There _still is_ a relationship between the probability of living below the poverty level and the time you wake up in the morning after controlling for gender. 

## 3. Fit the simple model
Fit the logistic regression model (a.k.a generalized linear model) of the explanatory variable on the response variable. Decide to reject the null hypothesis in favor of the alternative. 

```{r}
log.mod.1 <- glm(poverty~wakeup, data=addhealth, family='binomial')
summary(log.mod.1)
```

The p-value for the b1 estimate of the regression coefficient for `wakeup` is significant at 0.011. There is reason to believe that the time you wakeup is associated with the probability of living below the poverty level. 

 
## 4. Fit the multivariable model
Fit the same logistic regression model and include the potential confounding variable. **This is only done if there is a significant relationship between the explanatory and response variable.** Determine if the third variable is a confounder. 

```{r}
log.mod.2 <- glm(poverty~wakeup + female_c, data=addhealth, family='binomial')
pander(summary(log.mod.2))
```

The p-value for the regression coefficient estimte of `wakeup` is still significant at 0.0007 after controlling for gender. Thus gender is **not** a confounder. 


## 5. Interpret the Results

Print out the odds ratio estimates for the confounding model. You will see one of three things: 

* **OR = 1** = equal chance of response variable being YES given any explanatory variable value. You are not able to predict participants’ responses by knowing their explanatory variable value. This would be a nonsignificant model when looking at the p-value for the explanatory variable in the parameter estimate table.
* **OR > 1** = as the explanatory variable value increases, the presence of a YES response is more likely. We can say that when a participant’s response to the explanatory variable is YES (1), they are more likely to have a response that is a YES (1). 
* **OR <1** = as the explanatory variable value increases, the presence of a YES response is less likely. We can say that when a participant’s response to the explanatory variable is YES (1) they are less likely to have a response that is a YES (1). 


Interpreting confidence intervals for Odds Ratios

* Confidence intervals are a range for the population’s predicted odds ratio based on the sample data. We are 95% confident that any given population’s odds ratio would range between those two values. 
* When the confidence intervals for the explanatory variables and third variables do not overlap , the variable with the higher values we can interpret as being more strongly associated with our response variable.
* For both the odds ratio and confidence interval interpretation, when you add in third variables it is explained in the same way except that you are controlling for the third variable or explanatory variable.
* When you add a third variable to the logistic regression, if you determine one is a confound then you do not interpret the variable that becomes nonsignificant in the odds ratio.

```{r}   
# For your assignment - replace the saved model object `log.mod.2` with whatever YOU named this model. 
kable(
  data.frame(
    OR = exp(coef(log.mod.2)), 
    LCL = exp(confint(log.mod.2))[,1], 
    UCL = exp(confint(log.mod.2))[,2]
  ),
digits=2, align = 'ccc')
```

## 5. Conclusion

Replace the **bold** words with your variables, the `highlighted` words with data from your analysis, and choose between _conclusion options_. 


After adjusting for the potential confounding factor of **third variable**, **explanatory variable** (`OR odds ratio estimate, CI confidence interval range, p = significance value`) was _significantly/not significantly_ and _positively/negatively_ associated with the likelihood of **response variable**. In this analysis, the odds ratio tells us that those who are **[describe what dummy code 1 of your explanatory variable means here]** are `0.05` times _more (if OR greater than 1)/less (if OR less than 1)_ likely to **[describe what dummy code 1 of your response variable means here]**.  Based on these analyses, **third variable** _is not/is_ a confounding factor because the association between **explanatory** and **response** _is still/is no longer_ significant after accounting for **third variable**. 

So the conclusion for this analysis reads:

After adjusting for the potential confounding factor of **gender**, **wakeup time** (`0.11, CI 0.09-0.14, p = .0007`) was _significantly_ and _negatively_ associated with the likelihood of **earning under the poverty level**. In this analysis, the odds ratio tells us that those who are **female[describe what dummy code 1 of your explanatory variable means here]** are `odds ratio estimate` times _more (if OR greater than 1)/less (if OR less than 1)_ likely to **[describe what dummy code 1 of your response variable means here]**.  Based on these analyses, **third variable** _is not/is_ a confounding factor because the association between **explanatory** and **response** _is still/is no longer_ significant after accounting for **third variable**. 


Additional example interpretation

> * After adjusting for the potential confounding factor of gender, being overweight (OR 0.920, CI 0.822 – 1.028, p = .1420) was not significantly associated with the likelihood of participating in an active sport. In this analysis, the odds ratio tells us that those adolescents who are overweight are 0.920 times less likely to participate in an active sport. Based on these analyses, gender is a confounding factor because the association between being overweight and active sport participation is no longer significant after accounting for gender.
> * After adjusting for the potential confounding factor of gender, being overweight (OR 3.65, CI 1.573 – 4.891, p = .0001) was significantly and positively associated with the likelihood of participating in an active sport. In this analysis, the odds ratio tells us that those adolescents who are overweight are 3.65 times more likely to participate in an active sport. Based on these analyses, gender is not a confounding factor because the association between being overweight and active sport participation is still significant after accounting for gender. 
 


# Log Linear Regression

We are going to analyze personal income as the quantitative outcome for this piece of the assignment. **You must use this variable as your outcome for this question.** First however, you need to clean up, and log transform the variable for personal earnings `H4EC2` by following the steps below _in order_. 

1. Remove values above 999995 (structural missing). 
2. Create a binary indicator variable called `poverty`, where `poverty=1` if `H4EC2<10210` and 0 otherwise.
    * $10210 was the federal poverty limit of 2008. 
3. Create a new variable called `income`, that sets all values of personal income to be NA if below the federal poverty line. 
    - First set `income= H4EC2`
    - Then set income to missing, if `H4EC2 < 10210`
4. Then create a new variable: `logincome` that is the natural log (_ln_) of income. 

See the [sample data management file](dm_addhealth.html) for coding assistance. 


## 1. Identify variables

## 2. State hypothesis

## 3. Fit the simple model

## 4. Fit the multivariable model

## 5. Interpret the confounder

## 6. Conclusion

## Examples

# Adding Categorical predictors

For any of the regression models above, add a categorical variable with more than 2 levels.
* If the confounder was significant, use the multivariable model including confounder. 
* If the confounder was not significant, use the bivariate model without the confounder. 
* Interpret the regression coefficients for at least two levels of the categorical variable.  




# Model comparison via LRT

Using the model above, determine if adding the categorical variable improves the model. 

1. Report and compare the adjusted $R^{2}$ between the two predictor model and the three predictor model. 
2. Use a Likelihood Ratio Test to test if the additional variable improved the model fit. 

```{r, eval=FALSE}
anova(base, full, test="LRT")
```






