---
title: "Regression Assignment"
author: "Robin Donatello"
date: "Last Updated `r Sys.time()`"
output:
  html_document:
    highlight: tango
    theme: flatly
    toc: yes
    toc_float: yes
  pdf_document: default
css: ../css/customh5.css
---

# Assignment Overview

You will perform <span style="color:blue">3</span> regression analyses in this assignment. For each you will interpret the regression coefficients, and test for a potential confounder.

1. Multiple Linear Regression: $Q \sim Q + B$
2. Logistic Regression: $B \sim Q + B$
3. Log-Linear Regression: $log(Q) \sim Q + B$

You will then take one of the above analyses and 

4. Add and interpret categorical variables
5. Conduct a Likelihood Ratio Test to compare the model fit with and without the additional categorical variable. 


#### Submission Guidelines

* Use the template provided: [[RMD]]() for R users, and [[Word]]() for SPSS users. 
* Upload to the corresponding assignment on Blackboard by the due date. 
* Similar to the Bivariate Inference assignment, post a copy of this assignment to your blog as you complete each stage. 


# Instructions
1. Identify variables under consideration
    - Determine a third variable that you want to test as a potential confounder.  
    - Consider the relationship and ask yourself if I had to predict a future persons response based on the Predictor/Explanatory variable and some other variable or variables what would they be?
    - For the purposes of this assignment, your third variables must be binary. 
    - If you are using a “strongly agree” to “strongly disagree” (Likert scale) variable, decide if you want to compare those that “strongly agreed” on question to all those people that did not or if you want to compare those that “strongly disagreed” on question to all those people that did not. Then recode the original variable so that those that are “strongly agree” are dummy coded to 1 (i.e., yes to the statement) and all other levels are dummy coded to 0 (i.e., no to the statement).

2. State the research Hypothesis
    - Write out your Research Question, Null, and Alternative Hypothesis **for both analyses** you intend to run to determine if the Third Variable is a Confound or not. 
    - These will be written the same, except the second hypothesis will include the phrasing of “after controlling for ‘Third Variable’.” 
3. Fit two models
    - The first with just the response variable $y$ and the explanatory variable $x$ `lm(y ~ x)`
    - The second includes a third variable $z$. `lm(y ~ x + z)`
4. Interpret the Results
    - Details in each section below. 
5. Conclusion
    - Structured response template in each section below. 

# Multiple Linear Regression (Q ~ Q + B)

## 1. Identify variables under consideration

This will work best if your Q~Q relationship under consideration is significant. If you have a “Strongly Agree” to “Strongly Disagree” variable that you have kept all 5 levels, you can treat it as a Quantitative Variable.

 
## 2. State the research Hypothesis

Bivariate model: 
Null: There is no relationship between
Alternative: There is a relationship between

Multivariable model: 
Null: There is no relationship between
Alternative: There is a relationship between


## 3. Fit the models

```{r}
lm(Sepal.Width ~ Petal.Width, data=iris)
```

```{r}
lm(Sepal.Width ~ Petal.Width + Species, data=iris)
```


## 4. Interpret the Results

* Look at the first set of results. The relationship between the predictor and the response should be significant.
* Look at the Adjusted $R^{2}$ to see how much of the variance in the response you are accounting for with the predictor.
* Look at the second set of results. When the third varible is included in the model, look at the p-value for the predictor. One of two things will have happened.  
    - If it is still significant, the third variable is NOT a confounder because after controlling for the effects of the third variable from the relationship between the predictor and response it is still significant. When control for it, you are essentially putting everyone on the same level regarding that third variable.
    - If the p-value for explanatory variable is no longer significant, then the third variable is a confounding variable. This means that the third variable is explaining the relationship between the explanatory variable and the response variable.


## 5. Conclusion

Replace the **bold** words with your variables, the `highlighted` words with data from your analysis, and choose between _conclusion options_. 


After adjusting for the potential confounding factor of **third variable**, **explanatory variable** ($\beta_{1}$ = `parameter estimate, CI confidence interval range, p = significance value`) was ***significantly/not significantly** and **positively/negatively** associated with **response variable**. Approximately `R-Square*100` of the variance of **response** can be accounted for by **explanatory** after controlling for **third variable**. Based on these analyses, **third variable** _is not/is_ a confounding factor because the association between **explanatory** and **response** _is still/is no longer_ significant after accounting for **third variable**. 

## Examples

> * After adjusting for the potential confounding factor of gender, number of times an adolescent has skipped school (Beta = -0.021, 95% CI -0.03, -0.012, p = .0001) was significantly and negatively associated with GPA. Approximately 6.58% of the variance in GPA can be accounted for by skipping school after controlling for gender. Based on these analyses, gender is not a confounding factor because the association between skipping school and GPA is still significant after accounting for gender. 
> * After adjusting for the potential confounding factor of gender, an adolescent’s weight (Beta = 1.34, 95% CI -0.53, 3.21, p = .1558) was not significantly associated with the number of cigarettes smoked in the past 30 days. Approximately 0.78% of the variance in cigarettes smoked can be accounted for by weight after controlling for gender. Based on these analyses, gender is a confounding factor because the association between weight and cigarettes smoked is no longer significant after accounting for gender. 


# Logistic Regression (B ~ Q + B)

Your outcome variable must be coded as 1 (event) and 0 (non-event). Recoding this way ensures you are predicting the presence of your categorical variable and not the absence of it. 

## 1. Identify variables under consideration

## 2. State the research Hypothesis

## 3. Fit the models

```
log.mod.1 <- glm(y~x, data=, family='binomial')
summary(log.mod.1)
exp(coef(log.mod.1))
exp(confint(log.mod.1))
```

```
log.mod.2 <- glm(y~x+z, data=, family='binomial')
summary(log.mod.2)
exp(coef(log.mod.2))
exp(confint(log.mod.2))
```


## 4. Interpret the Results

* Look to the first set of results. The relationship between the explanatory and response variable should be significant in the “analysis of maximum likelihood” table (9th table down). If so, then you can test third variables to see if they are confounds or predictors. There is no $R^{2}$ here. 
* Now look to the second set of results. When the third variable is included in the code, look at the p-value for the explanatory variable. If it is still significant, the third variable is not a confounding variable. 
* In other words, after controlling for the effects of the third variable on the relationship between the explanatory and response variable, the relationship is still significant. When you control for a variable, you are essentially putting everyone on the same level regarding that variable. 
* Next, look at the “odds ratio estimates” table (10th table down). Remember you must have recoded your variables correctly for logistic regression to be able to interpret this. You will see one of three things: 
    - OR estimate = 1 = equal chance of response variable being YES given any explanatory variable value. You are not able to predict participants’ responses by knowing their explanatory variable value. This would be a nonsignificant model when looking at the p-value for the explanatory variable in the parameter estimate table.
    - OR estimate greater than 1 = as the explanatory variable value increases, the presence of a YES response is more likely. We can say that when a participant’s response to the explanatory variable is YES (1), they are more likely to have a response that is a YES (1). 
    - OR estimate less than 1 = as the explanatory variable value increases, the presence of a YES response is less likely. We can say that when a participant’s response to the explanatory variable is YES (1) they are less likely to have a response that is a YES (1). 
* Confidence intervals are a range for the population’s predicted odds ratio based on the sample data. We are 95% confident that any given population’s odds ratio would range between those two values. When the confidence intervals do not overlap for the explanatory variables and third variables, the variable with the higher values we would say is more strongly associated with our response variable.
* For both the odds ratio and confidence interval interpretation, when you add in third variables it is explained in the same way except that you are controlling for the third variable or explanatory variable.
* When you add a third variable to the logistic regression, if you determine one is a confound then you do not interpret the variable that becomes nonsignificant in the odds ratio.


## 5. Conclusion

Replace the **bold** words with your variables, the `highlighted` words with data from your analysis, and choose between _conclusion options_. 


After adjusting for the potential confounding factor of **third variable**, **explanatory variable** (`OR odds ratio estimate, CI confidence interval range, p = significance value`) was _significantly/not significantly_ and _positively/negatively_ associated with the likelihood of **response variable**. In this analysis, the odds ratio tells us that those who are **[describe what dummy code 1 of your explanatory variable means here]** are `odds ratio estimate` times _more (if OR greater than 1)/less (if OR less than 1)_ likely to **[describe what dummy code 1 of your response variable means here]**.  Based on these analyses, **third variable** _is not/is_ a confounding factor because the association between **explanatory** and **response** _is still/is no longer_ significant after accounting for **third variable**. 


## Examples

> * After adjusting for the potential confounding factor of gender, being overweight (OR 0.920, CI 0.822 – 1.028, p = .1420) was not significantly associated with the likelihood of participating in an active sport. In this analysis, the odds ratio tells us that those adolescents who are overweight are 0.920 times less likely to participate in an active sport. Based on these analyses, gender is a confounding factor because the association between being overweight and active sport participation is no longer significant after accounting for gender.
> * After adjusting for the potential confounding factor of gender, being overweight (OR 3.65, CI 1.573 – 4.891, p = .0001) was significantly and positively associated with the likelihood of participating in an active sport. In this analysis, the odds ratio tells us that those adolescents who are overweight are 3.65 times more likely to participate in an active sport. Based on these analyses, gender is not a confounding factor because the association between being overweight and active sport participation is still significant after accounting for gender. 
 


# Log Linear Regression (log(Q) + Q + B)

We are going to analyze personal income as the quantitative outcome for this piece of the assignment. **You must use this variable as your outcome for this question.** First however, you need to clean up, and log transform the variable for personal earnings `H4EC2` by following the steps below _in order_. 

1. Remove values above 999995 (structural missing). 
2. Create a binary indicator variable called `poverty`, where `poverty=1` if `H4EC2<10210` and 0 otherwise.
    * $10210 was the federal poverty limit of 2008. 
3. Create a new variable called `income`, that sets all values of personal income to be NA if below the federal poverty line. 
    - First set `income= H4EC2`
    - Then set income to missing, if `H4EC2 < 10210`
4. Then create a new variable: `logincome` that is the natural log (_ln_) of income. 

See the [sample data management file](dm_addhealth.html) for coding assistance. 


## 1. Identify variables under consideration

## 2. State the research hypothesis

## 3. Fit the model

## 4. Interpret the results

## 5. Conclusion

## Examples

# Categorical predictors


For either your Multiple Linear Regression, or your Log-Linear regression models above, add a categorical variable with 3 levels. Interpret the regression coefficients for the categorical variables.  


# Model fit

Using the model above, determine if adding the categorical variable improves the model. 

1. Report and compare the adjusted $R^{2}$ between the two predictor model and the three predictor model. 
2. Use a Likelihood Ratio Test to test if the additional variable improved the model fit. 

```{r, eval=FALSE}
anova(base, full, test="LRT")
```






