---
title: 'Logistic Regression'
author: "Robin Donatello"
date: "Last Updated `r Sys.time()`"
output:
  pdf_document:
    toc: yes
  html_document:
    highlight: tango
    theme: flatly
    toc: yes
    toc_float: yes
css: ../css/customh5.css
---
```{r,echo=FALSE, warning=FALSE, message=FALSE}
library(knitr); library(rmarkdown);
library(ggplot2); library(dplyr); 
library(xtable); library(pander)
options(xtable.comment = FALSE)
opts_chunk$set(warning=FALSE, message=FALSE) 
```


# References

## Assigned reading

* Open Intro Section 8.4
* PMA5 Ch 12 (selected)
* Article: When can odds ratios mislead? http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1112884/

## Additional references
* Odds Ratios: http://www.ats.ucla.edu/stat/sas/faq/oratio.htm 
* Marin Stats Lecture on OR and RR: https://www.youtube.com/watch?v=V_YNPQoAyCc 


# Introduction

* Logistic regression is a tool used to model a categorical outcome variable with two levels: Y = 1 if event, = 0 if no event. 
* Instead of modeling the outcome directly $E(Y|X)$ as with linear regression, we model the probability of an event occurring: $P(Y=1|X)$. 

## Uses of Logistic Regression (PMA5 12.10)
* Assess the impact selected covariates have on the probability of an outcome occurring. 
* Predict the likelihood / chance / probability of an event occurring given a certain covariate pattern.  



# The Logistic Regression Model (PMA5 12.4)
Let $p_{i} = P(y_{i}=1)$. 

The logistic model relates the probability of an event based on a linear combination of X's. 

$$
log\left(
\frac{p_{i}}{1-p_{i}}
\right) = \beta_{0} + \beta_{1}x_{1i} + \beta_{2}x_{2i} + \ldots + \beta_{p}x_{pi}
$$

Since the _odds_ are defined as the probability an event occurs divided by the  probability it does not occur: $(p/(1-p))$, the function $log\left(\frac{p_{i}}{1-p_{i}}\right)$ is also known as the _log odds_, or more commonly called the **_logit_**. 

```{r, fig.width=4, fig.height=3}
p <- seq(0, 1, by=.01)
logit.p <- log(p/(1-p))
qplot(logit.p, p, geom="line", xlab = "logit(p)", main="The logit transformation")
```

This in essence takes a binary outcome 0/1 variable, turns it into a continuous probability (which only has a range from 0 to 1) Then the logit(p) has a continuous distribution ranging from $-\infty$ to $\infty$, which is the same form as a Multiple Linear Regression (continuous outcome modeled on a set of covariates)

## Modeling the probability of an event. 

Back solving the logistic model for $p_{i} = e^{\beta X} / (1+e^{\beta X})$: 

$$
p_{i} = \frac{e^{\beta_{0} + \beta_{1}x_{1i} + \beta_{2}x_{2i} + \ldots + \beta_{p}x_{pi}}}
{1 + e^{\beta_{0} + \beta_{1}x_{1i} + \beta_{2}x_{2i} + \ldots + \beta_{p}x_{pi}}}
$$

## Logistic Regression via GLM

A logistic regression model can be fit in R using the `glm()` function. GLM stands for _Generalized Linear Model_. GLM's can fit an entire _family_ of distributions and can be thought of as $E(Y|X) = C(X)$ where $C$ is a **link** function that relates $Y$ to $X$.

* Linear regression: C = Identity function (no change)
* Logistic regression: C = logit function
* Poisson regression: C = log function

The outcome $y$ is a 0/1 Bernoulli random variable. The sum of a vector of Bernoulli's ($\sum_{i=1}^{n}y_{i}$) has a Binomial distribution. When we specify that `family = "binomial"` the `glm()` function auto-assigns "logit" link function. See `?family` for more information on this. 


```{r, eval=FALSE}
glm(y ~ x1 + x2 + x3, data=DATA, family="binomial")
```


##### Example: Gender effects on Depression

Is gender associated with depression? Read in the `depression` data and recode sex to be an indicator of being male. 
```{r}
depress <- read.delim("https://norcalbiostat.netlify.com/data/depress_081217.txt")
names(depress) <- tolower(names(depress)) # make all variable names lower case. 
```

* Binary outcome variable: Symptoms of Depression (`cases`)
* Binary predictor variable: Gender (`sex`) as an indicator of being female

We fit the logistic regression model using a _generalized linear model_, specifying that the `family=binomial`. This tells `R` to use a _logit_ link on the linear combination. SPSS users will choose the LOGISTIC function. 

```{r}
dep_sex_model <- glm(cases ~ sex, data=depress, family="binomial")
summary(dep_sex_model)
```

We exponentiate the coefficients to back transform the $\beta$ estimates into Odds Ratios
```{r}
exp(coef(dep_sex_model))
```

Females have 2.8 times the odds of showing signs of depression compared to males. 

## Confidence Intervals
The OR is **not** a linear function of the $x's$, but $\beta$ is. This means that a CI for the OR is created by calculating a CI for $\beta$, and then exponentiating the endpoints. A 95% CI for the OR can be calculated as: 

$$e^{\hat{\beta} \pm 1.96 SE_{\beta}} $$

```{r}
exp(confint(dep_sex_model))
```


# Multiple Logistic Regression (PMA5 12.5, 12.6)
Just like multiple linear regression, additional predictors are simply included in the model using a `+` symbol. 
```{r}
mvmodel <- glm(cases ~ age + income + sex, data=depress, family="binomial")
summary(mvmodel)
```

* The sign of the $\beta$ coefficients can be interpreted in the same manner as with linear regression. 
* The odds of being depressed are less if the respondent has a higher income and is older, and higher if the respondent is female. 

### OR interpretation

* The OR provides a directly understandable statistic for the relationship between $y$ and a specific $x$ given all other $x$'s in the model are fixed. 
* For a continuous variable X with slope coefficient $\beta$, the quantity $e^{b}$ is interpreted as the ratio of the odds for a person with value (X+1) relative to the odds for a person with value X. 
* $exp(kb)$ is the incremental odds ratio corresponding to an increase of $k$ units in the variable X, assuming that the values of all other X variables remain unchanged. 

**Binary variables**
Calculate the Odds Ratio of depression for women compared to men. 

**WRITE OUT THE MODEL**
$$log(odds) = -0.676 - 0.02096*age - .03656*income + 0.92945*gender$$

$$ OR = \frac{Odds (Y=1|F)}{Odds (Y=1|M)} $$

Write out the equations for men and women separately. 
$$ = \frac{e^{-0.676 - 0.02096*age - .03656*income + 0.92945(1)}}
          {e^{-0.676 - 0.02096*age - .03656*income + 0.92945(0)}}$$

Applying rules of exponents to simplify.
$$ = \frac{e^{-0.676}e^{- 0.02096*age}e^{- .03656*income}e^{0.92945(1)}}
          {e^{-0.676}e^{- 0.02096*age}e^{- .03656*income}e^{0.92945(0)}}$$

$$ = \frac{e^{0.92945(1)}}
          {e^{0.92945(0)}}$$

$$ = e^{0.92945} $$

```{r}
exp(.92945)
exp(coef(mvmodel)[4])
```

The odds of a female being depressed are 2.53 times greater than the odds for Males after adjusting for the linear effects of age and income (p=.016). 

**Continuous variables**

```{r}
exp(coef(mvmodel))
exp(confint(mvmodel))
```

* The Adjusted odds ratio (AOR) for increase of 1 year of age is 0.98 (95%CI .96, 1.0)
* How about a 10 year increase in age? $e^{10*\beta_{age}} = e^{-.21} = .81$
```{r}
exp(10*coef(mvmodel)[2])
```
with a confidence interval of
```{r}
round(exp(10*confint(mvmodel)[2,]),3)
```
Controlling for gender and income, an individual has 0.81 (95% CI 0.68, 0.97) times the odds of being depressed compared to someone who is 10 years younger than them. 

# Interaction terms (PMA5 12.7)

The inclusion of an interaction is necessary if the effect of an independent variable depends on the level of another independent variable.

##### Example: The relationship between income, employment status and depression. 

Here I create the binary indicators of lowincome and underemployed as described in the textbook. In each case I ensure that missing data is retained.  
```{r}
depress$lowincome <- ifelse(depress$income < 10, 1, 0)
depress$lowincome <- ifelse(is.na(depress$income), NA, depress$lowincome)

depress$underemployed <- ifelse(depress$employ %in% c(2,3), 1, 0 )
depress$underemployed <- ifelse(is.na(depress$employ) | depress$employ==7, NA, depress$underemployed)
table(depress$underemployed, depress$employ, useNA="always")
```

The **Main Effects** model assumes that the effect of income on depression is independent of employment status, and the effect of employment status on depression is independent of income. 

```{r}
me_model <- glm(cases ~ lowincome + underemployed, data=depress, family="binomial")
summary(me_model)
```

To formally test whether an interaction term is necessary, we add the interaction term into the model and assess whether the coefficient for the interaction term is significantly different from zero. 
```{r}
summary(glm(cases ~ lowincome + underemployed + lowincome*underemployed, data=depress, family="binomial"))
```

# CAUTION

Consider a hypothetical example where the probability of death is .4 for males and .6 for females. 

The odds of death for males is `.4/(1-.4)` = `r round(.4/.6,2)`.
The odds of death for females is `.6/(1-.6)` = `r round(.6/.4,2)`.

The Odds Ratio of death for females compared to males is ` 1.5/.66` = `r round(1.5/.66,2)`.

* If you were to say that females were 2.3 times as likely to die compare to males, you wouldn't necessarily translate that to a 40% vs 60% chance. 


## Probability Interpretation
For the above model of depression on age, income and gender the probability of depression is: 
$$
P(depressed) = \frac{e^{-0.676 - 0.02096*age - .03656*income + 0.92945*gender}}
{1 + e^{-0.676 - 0.02096*age - .03656*income + 0.92945*gender}}
$$

Let's compare the probability of being depressed for males and females separately, while holding age and income constant at their average value. 

```{r}
depress %>% summarize(age=mean(age), income=mean(income))
```

Plug the coefficient estimates and the values of the variables into the equation and calculate. 
$$
P(depressed|Female) = \frac{e^{-0.676 - 0.02096(44.4) - .03656(20.6) + 0.92945(1)}}
{1 + e^{-0.676 - 0.02096(44.4) - .03656(20.6) + 0.92945(1)}}
$$

```{r}
XB.f <- -0.676 - 0.02096*(44.4) - .03656*(20.6) + 0.92945
exp(XB.f) / (1+exp(XB.f))
```
$$
P(depressed|Male) = \frac{e^{-0.676 - 0.02096(44.4) - .03656(20.6) + 0.92945(0)}}
{1 + e^{-0.676 - 0.02096(44.4) - .03656(20.6) + 0.92945(0)}}
$$
```{r}
XB.m <- -0.676 - 0.02096*(44.4) - .03656*(20.6)
exp(XB.m) / (1+exp(XB.m))
```

The probability for a 44.4 year old female who makes $20.6k annual income has a 0.19 probability of being depressed. The probability of depression for a male of equal age and income is 0.86. 

# Relative Risk and 2x2 tables

An Odds Ratio is one measure of association between two binary variables, the **Relative Risk Ratio** is another measure. Both can be calculated on a 2x2 contingency table. Note that the OR that is generated from a GLM with only a binary categorical predictor will be identical to the one calculated on the 2x2 table. GLM's have more flexibility for further model building, which is why it is introduced first. 

Consider a 2x2 contingency table similar to the following: 

```{r, results='asis'}
nnnn <- matrix(c("$n_{11}$", "$n_{12}$", "$n_{1.}$", 
                 "$n_{21}$", "$n_{22}$", "$n_{2.}$", 
                 "$n_{.1}$", "$n_{.2}$", "$n_{..}$"), nrow=3, byrow=TRUE,  
          dimnames = list(c("Exposed", "Not-Exposed", "Total"), c("Diseased", "Not-Diseased", "Total")))
#print(xtable(nnnn, align='cccc'), type='html')
```

```{r}
pander(nnnn)
```


Sometimes the cell contents are abbreviated as:
```{r, results="asis"}
abcd <- matrix(c("a", "b", "c", "d"), nrow=2,  
          dimnames = list(c("Exposed", "Not-Exposed"), c("Diseased", "Not-Diseased")))
#print(xtable(abcd, align='ccc'), type="html")
```

```{r}
pander(abcd)
```


<br></br>

## Relative Risk

The **Relative Risk (RR)** or **Risk Ratio** is the ratio of the probability of an event occurring in an exposed group compared to the probability of an event occurring in a non-exposed group. 

* Asymptotically approaches the OR for small probabilities. 
* Often used in cohort studies and randomized control trials. 

Consider sample proportions Diseases within Exposed and Non-exposed groups.
$$\hat{\pi}_{1} = \frac{n_{11}}{n_{1.}} \qquad \mbox{and} \qquad \hat{\pi}_{2} = \frac{n_{21}}{n_{2.}}$$

The Relative Risk is calculated as 

$$RR = \frac{\hat{\pi}_{1}}{\hat{\pi}_{2}} \qquad \mbox{or} \qquad \frac{a/(a+b)}{c/(c+d)}$$

with variance 
$$V = \frac{1-\hat{\pi}_{1}}{n_{11}} + \frac{1-\hat{\pi}_{2}}{n_{21}}$$


## Odds Ratio 

The **Odds Ratio (OR)** is a way to quantify how strongly the presence or absence of a characteristic affects the presence or absence of a second characteristic. 

* Often used in case-control studies
* The main interpretable estimate generated from logistic regression

The **Odds of an event** is the probability it occurs divided by the probability it does not occur. 

$$ odds_{1} = \frac{n_{11}/n_{1.}}{n_{12}/n_{1.}} = \frac{n_{11}}{n_{12}} $$
$$ odds_{2} = \frac{n_{21}/n_{2.}}{n_{22}/n_{2.}} = \frac{n_{21}}{n_{22}} $$

The **Odds Ratio** for group 1 compared to group 2 is the ratio of the two odds written above: 

$$OR = \frac{odds_{1}}{odds_{2}} = \frac{n_{11}n_{22}}{n_{12}n_{21}} \qquad \mbox{or} \qquad \frac{ad}{bc}$$

with variance $V = n^{-1}_{11} + n^{-1}_{12} + n^{-1}_{21} + n^{-1}_{22}$. 

## Confidence Intervals

Neither the Risk Ratio nor the Odds Ratio are linear functions, so a 95% CI for the population estimates are not your typical $\hat{\theta} \pm 1.96\sqrt{Var(\hat{\theta})}$.

Instead they are calculated as the point estimate $\hat{\theta}$ times $e$ raised to the $\pm 1.96$ times the standard deviation of the estimate. 

$$(\hat{\theta}e^{-1.96*\sqrt{V}}, \hat{\theta}e^{1.96*\sqrt{V}})$$


#### Example: Are females more likely to show signs of depression than males? 

```{r}
table(depress$sex, depress$cases, dnn=c("Female", "Signs of Depression"))
```

Note that both the columns and rows are swapped when compared to the a/b/c/d format. 
For ease of interpretation I will recreate the table manually. 
```{r}
tab_sn <- matrix(c(101, 10, 143, 40), nrow=2, byrow=T, 
                 dimnames = list(c("Male", "Female"), c("No signs", "Signs")))
tab_sn
```

Now I use the `epi.2by2` function contained in the `epiR` package to calculate
the Odds Ratio, the Risk Ratio, and their respective confidence intervals. 
```{r}
library(epiR)
epi.2by2(tab_sn)
```

* Females are 1.16 (1.06, 1.28) times as likely as men to show signs of depression. 
* Females have 2.83 (1.35, 5.91) times the odds of showing signs of depression compared to males. 

Both intervals are greater than 1, therefore the event (depressive signs) is statistically
more likely to occur in the exposed group (female) than in the control (males) (p=.004). 
 

* Mathematical reference for the Wald test Statistic http://www.statlect.com/Wald_test.htm 






