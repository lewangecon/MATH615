---
title: "Multiple Regression"
author: "Robin Donatello"
date: "Last Updated `r Sys.time()`"
output:
  html_document:
    highlight: tango
    theme: flatly
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
css: ../css/customh5.css
---
```{r,echo=FALSE, warning=FALSE, message=FALSE}
library(knitr); library(rmarkdown);library(ggplot2)
opts_chunk$set(warning=FALSE, message=FALSE, fig.height=4, fig.width=5) 
```


Consider an observational study where researchers wanted to know if certain drugs, or certain therapy, or some combination thereof worked best to help participants quit smoking. They determined that the percentage succeeding with the combination drug/therapy method was highest, while the percentage succeeding with neither therapy nor drugs was lowest. In other words, suppose there is clear evidence of an association between method used and success rate. Could they then conclude that the combination drug/therapy method causes success more than using neither therapy nor a drug?


## Aims
* Extend simple linear regression.
* Describe linear relationship between a single continuous $Y$ variable, 
  and several $X$ variables.
* Draw inferences regarding this relationship.
* Predict $Y$ from $X_{1}, X_{2}, \ldots , X_{P}$.

Now it's no longer a 2D regression _line_, but a $p$ dimensional 
regression plane. 

![Figure 7.1](regression_plane.png)

## Types of X variables
* Fixed: The levels of $X$ are selected in advance with the intent to measure
  the affect on an outcome $Y$. 
* Variable: Random sample of individuals from the population is taken and $X$
  and $Y$ are measured on each individual. 
* X's can be continuous or discrete (categorical)
* X's can be transformations of other X's, e.g., polynomial regression

## Mathematical Model

$$ y_{i} = \beta_{0} + \beta_{1}x_{1i} + \ldots + \beta_{p}x_{pi} + \epsilon_{i}$$

The assumptions on the residuals $\epsilon_{i}$ still hold:   

* They have mean zero  
* They are homoscedastic, that is all have the same finite variance: $Var(\epsilon_{i})=\sigma^{2}<\infty$  
* Distinct error terms are uncorrelated: (Independent) $\text{Cov}(\epsilon_{i},\epsilon_{j})=0,\forall i\neq j.$  

The regression model relates $y$ to a function of $\textbf{X}$ and $\mathbf{\beta}$, 
where $\textbf{X}$ is a $nxp$ matrix of $p$ covariates on $n$ observations and 
$\mathbf{\beta}$ is a length $p$ vector of regression coefficients. In matrix notation
this looks like: 

$$ \textbf{y} = \textbf{X} \mathbf{\beta} + \mathbf{\epsilon} $$

### Parameter Estimation
The goal of regression analysis is to minimize the residual error. 
That is, to minimize the difference between the value of the dependent
variable predicted by the model and the true value of the dependent variable.

$$ \epsilon_{i} = \hat{y_{i}} - y_{i}$$

The method of least squares accomplishes this by finding parameter estimates 
$\beta_{0}$ and $\beta_{1}$ that minimized the sum of the squared residuals:

$$ \sum_{i=1}^{n} \epsilon_{i} $$

For simple linear regression these are found to be 
$$ \hat{\beta_{0}} = \bar{y} * \hat{\beta_{1}}\bar{x} \quad \mbox{  and  } \quad  \hat{\beta_{1}} = r\frac{s_{y}}{s_{x}} $$

For multiple linear regression the function to minimize is

$$ \sum_{i=1}^{n} |y_{i} - \sum_{i=1}^{p}X_{ij}\beta_{j}|^{2}$$

Or in matrix notation

$$ || \mathbf{y} - \mathbf{X}\mathbf{\beta} ||^{2} $$ 

The details of methods to solve these minimization functions are left to a
course in mathematical statistics, however we will return to this notation. 

### Continued Example: Lung Function

In Chapter 6 the data for fathers from the lung function data set were 
analyzed. These data fit the variable-X case. Height was used as the 
$X$ variable in order to predict `FEV`. 

```{r}
fev <- read.delim("https://norcalbiostat.netlify.com/data/Lung_081217.txt", sep="\t", header=TRUE)
summary(lm(FFEV1 ~ FHEIGHT , data=fev))
round(confint(lm(FFEV1 ~ FHEIGHT , data=fev)),2)
```
This model concludes that FEV1 in fathers significantly increases by 0.12 (95% CI:
0.09, 0.15) liters per additional inch in height (p<.0001). Looking at the 
multiple $R^{2}$ (correlation of determination), this simple model explains 
25% of the variance seen in the outcome $y$. 

However, FEV tends to decrease with age for adults, so we should be able
to predict it better if we use both height and age as independent variables
in a multiple regression equation. 

First let's see different ways to graphically explore the relationship between
three characteristics simultaneously.

#### 3D scatterplots
```{r}
library(scatterplot3d)
scatterplot3d(x=fev$FHEIGHT, y=fev$FAGE, z=fev$FFEV1, 
              xlab="Height", ylab="Age", zlab="FEV", 
              main="Relationship between FEV1, height and age for fathers.")
```

#### Controlling the color, or size of points using the third characteristic
```{r, fig.width=10}
library(gridExtra)
a <- qplot(y=FFEV1, x=FAGE, color=FHEIGHT, data=fev)
b <- qplot(y=FFEV1, x=FHEIGHT, size=FAGE, data=fev)
grid.arrange(a, b, ncol=2)
```

The scatterplot of FEV against age demonstrates the decreasing trend of
FEV as age increases, and the increasing trend of FEV as height increases. 
The third color however is pretty scattered across the plot. There is no
obvious trend observed.

*  What direction do you expect the slope coefficient for age to be? For height? 

## Model fitting

### Simple Linear Regression
Let's examine the bivarate relationship of FEV1 (forced expiratory volume in 1 minute)
for fathers `FFEV1` on their age `FAGE`. 

```{r}
summary(lm(FFEV1 ~ FAGE, data=fev))
```

For every one year older the father gets, his FEV1 significantly decreases by 
0.03 liters (p = .00001).

```{r}
summary(lm(FFEV1 ~ FHEIGHT, data=fev))
```
For every inch taller a father is, his FEV1 significantly increases by 
0.11 liters (p < .0001).

### Multiple Linear Regression
Fitting a regression model in R with more than 1 predictor is trivial. Just add
each variable to the right hand side of the model notation connected with a `+`. 

```{r}
mv_model <- lm(FFEV1 ~ FAGE + FHEIGHT, data=fev)
summary(mv_model)
```
A father who is one year older is expected to have a FEV value 0.03 liters 
less than another father of the same height (p<.0001).

A father who is the same age as another father is expected to have a FEV
value of 0.11 liter greater than another father of the same age who is one inch shorter (p<.0001). 

For the model that includes age, the coefficient for height is now 
`r round(mv_model$coefficients[3],2)`, which is interpreted as the rate of change 
of FEV1 as a function of height **after adjusting for age**. 
This is also called the **partial regression coefficient** of FEV1 on height after 
adjusting for age. 

Both height and age are significantly associated with FEV in fathers (p<.0001 each).

### ANOVA for regression
For a global test to see whether or not the regression model is helpful in 
predicting the values of $y$, we can use an ANOVA. This is the same as testing
that all $\beta_{j}, j=1, \ldots, p$ are all equal to 0. Let's look at what
we get if we wrap the ANOVA function, `aov()` around the linear model results. 

```{r}
summary(aov(mv_model))
```

We get the sums of squares (SS) for each predictor individually, not combined
into a SS for regression and a SS residuals. So where do we find this
global test that this model is better than using no predictors at all? 
At the very last line in the summary of the linear model results. 

```{r}
summary(mv_model)
```


## Model Diagnostics 

The same set of regression diagnostics can be examined to identify
any potential influential points, outliers or other problems with
the linear model. 

```{r}
par(mfrow=c(2,2))
plot(mv_model)
```


### Multicollinearity

* Occurs when some of the X variables are highly intercorrelated.
* Affects estimates and their SE's (p. 143)
* Look at tolerance, and its inverse, the Variance Inflation Factor (VIF)
* Need tolerance < 0.01, or VIF > 100.

```{r}
library(car)
vif(mv_model)
tolerance = 1/vif(mv_model)
tolerance
```

* Solution: use variable selection to delete some X variables.
* Alternatively, use Principal Components (Ch. 14)

## Interaction Models

Consider a model with only two predictors: $X_{i}$ and $x_{j}$.

$$E(y|x_{i}) = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} $$

A model of this form is said to be _additive_. If the additive terms for 
these variables do not completely specify their effects on the dependent 
variable $Y$, then interaction of $X_{1}$ and $X_{2}$ is said to be present. 

#### Interlude: Reshaping the Lung data from wide to long. 

The data on Lung function came to us in _wide_ format, with separate 
variables for mother's and father's FEV1 score (`MFEV1` and `FFEV`). 
To analyze the effect of gender on FEV, the data need to be in _long_
format, with a single variable for `FEV` and a separate variable for
gender. The following code chunk demonstrates one method of combining
data on height, gender, age and FEV1 for both males and females. 

```{r}
fev2 <- data.frame(gender = c(fev$FSEX, fev$MSEX), 
                   FEV = c(fev$FFEV1, fev$MFEV1), 
                   ht = c(fev$FHEIGHT, fev$MHEIGHT), 
                   age = c(fev$FAGE, fev$MAGE))
fev2$gender <- factor(fev2$gender, labels=c("M", "F"))
head(fev2)  
```

What does the relationship between height and FEV look like for the overall sample? 
```{r}
ggplot(data=fev2, aes(x=ht, y=FEV)) + geom_point() + xlab("Height") + 
  ggtitle("Scatterplot of FEV vs height of adults") + 
  geom_smooth(se=FALSE, method='lm', col="purple") + 
  geom_smooth(se=FALSE, col="brown") 
```

However if we examine the relationship separately by gender we see
a slightly different story. 
```{r}
ggplot(data=fev2, aes(x=ht, y=FEV, col=gender)) + geom_point() + xlab("Height") + 
  ggtitle("Scatterplot of FEV vs height of adults by gender") + 
  geom_smooth(se=FALSE, method='lm') + 
  geom_smooth(aes(x=ht, y=FEV), col="purple", se=FALSE, method='lm')
```

If we put numeric summaries to these bivariate relationships we see that the
correlation between FEV and height overall is
`cor(fev2$FEV, fev2$ht)` = `r round(cor(fev2$FEV, fev2$ht),2)`, but for
males it is
`cor(fev$FFEV, fev$FHEIGHT)` = `r round(cor(fev$FFEV, fev$FHEIGHT),2)`, 
and for females the correlation is
`cor(fev$MFEV, fev$MHEIGHT)` = `r round(cor(fev$MFEV, fev$MHEIGHT),2)`.


**Conclusion:** The relationship between FEV1 and height may depend on gender. 
These variables are said to **interact** with each other. In other words, 
gender changes the relationship between height and FEV1. 

Consider the linear model of FEV on gender($x_{1}$), height($x_{2}$) and age($x_{3}$)
where gender interacts with  height. 

$$ FEV1 \sim \beta_{0} + \beta_{1}*gender + \beta_{2}*height + \beta_{3}*age + \beta_{4}*gender*height $$

According to the codebook, when `gender = 0` the record is on a male, and when `gender = 1` the record is
on a female. The model for males then simplifies to: 

$$ FEV1 \sim \beta_{0} + \beta_{2}*height + \beta_{3}*age $$

and the model for females would be:

$$ FEV1 \sim (\beta_{0} + \beta_{1}) + (\beta_{2} + \beta_{4})*height + \beta_{3}*age$$

Interactions are fit in `R` by simply multiplying `*` the two variables together in the model statement. 
```{r}
intx.model <- lm(FEV ~ gender + ht + age + gender*ht, data=fev2)
summary(intx.model)
confint(intx.model)
```

* Using the output from R after fitting the interaction model specified above, write out
the regression equation for males and females separately. 


The first thing to notice is the coefficient label: `genderF`. Since the
variable for gender is a factor variable, is automatically _reference coded_ 
into a new variable that is not present on the data set but only as part of the 
linear model function. This variable `genderF` is a 1 if the gender on record 
is female, and 0 otherwise. 

The p-value for the interaction term `ht:genderF` is large, and the confidence
interval for this parameter covers zero, so there is no indication that an 
interaction exists. That is, there is not enough reason to believe that gender
significantly affects the relationship between height and FEV. 

**Reminder** Just as in a Two-Way ANOVA with an interaction term, the main effects
cannot be interpreted directly when there is an interaction in the model. This means
you **cannot** interpret the direct effect of height or gender on FEV1 using just 
$\beta_{1}$ or $\beta_{3}$.


* What is the effect of height on FEV1 for females? 


Later on we will talk about how categorical variables with multiple factor
levels are reference coded for entry into models. 


   
# Categorical Predictors

## Factor variable coding

* Most commonly known as "Dummy coding"
* Better used term: Indicator variable
* Math notation: **I(gender == "Female")**. 
* A.k.a reference coding
* For a nominal X with K categories, define K indicator variables.
    - Choose a reference (referent) category:
    - Leave it out
    - Use remaining K-1 in the regression.
    - Often, the largest category is chosen as the reference category.


## Example: Religion against income and depression
Consider a log-linear model for the effect of marital status ($X_2$) on
log income while controlling for age($X_1$). This is called a log-linear
model because the outcome has been log transformed. 

$$ log(Y_i) = \beta_0 + \beta_1*x_1 + \beta_2*x_2 $$

Forget why we log-transformed income? Re-visit the notes on Data Screening 
and Transformations http://norcalbiostat.github.io/MATH456/notes/lec01_data_prep.html 

```{r}
dep <- read.table("https://norcalbiostat.netlify.com/data/depress_081217.txt", sep="\t", header=TRUE)
levels(dep$marital)
```

Marital status has 5 levels, so we would need 4 indicator variables. 
R always uses the first level of a factor variable as the reference level. 

* Let $x_{2}=1$ when `marital='Married'`, and 0 otherwise,  
* let $x_{3}=1$ when `marital='Never Married'`, and 0 otherwise,  
* let $x_{4}=1$ when `marital='Separated'`, and 0 otherwise,  
* let $x_{5}=1$ when `marital='Widowed'`, and 0 otherwise. 

The mathematical model would look like: 

$$ log(Y)|X \sim \beta_{0} + \beta_{1}*x_{1} + \beta_{2}x_{2} + \beta_{3}x_{3} + \beta_{4}x_{4} + \beta_{5}x_{5} $$

Two levels of interpretation here. 

1. The outcome is log transformed, so the interpretation has to be 
   back-transformed.   
2. The coefficients for the other levels of the categorical variable
   are in _comparison_ to the reference level. 


**Interpretation of log-linear models** 
Calculate the change in $Y$ that corresponds to a one unit change in $x_1$. 
Since marital status is remaining constant, I will exclude it from the 
calculations below to save space and not to detract from the main point.

Write each equation down

$$ log(Y)|x_1 = \beta_{0} + \beta_{1}x_{1}$$ 
$$ log(Y)|(x_1+1)  = \beta_{0} + \beta_{1}(x_{1}+1)$$ 

Find the difference

$$ (log(Y)|x_1) - (log(Y)|(x_1+1)) = (\beta_{0} + \beta_{1}x_{1}) - (\beta_{0} + \beta_{1}(x_{1}+1))$$ 

and simplify. 

$$ log(\frac{Y|x_1}{Y|x_1+1}) = \beta_{1}$$ 
$$ \frac{Y|x_1}{Y|x_1+1} = e^{\beta_{1}}$$

Each 1-unit increase in $x_{j}$ multiplies the expected value of Y by $e^{\hat{\beta_{j}}}$.  

Interpretation: $100\hat{\beta_{j}}$ is the expected **percentage** change
in $Y$ for a unit increase in $x_{j}$.


The nice thing about factor variables in R, is that the appropriate 
indicator variables are automatically created for you by the linear
model (`lm()`) function.

```{r, echo=1}
summary(lm(log(income) ~ age + marital,data=dep))
cf <- round(exp(coef(summary(lm(log(income) ~ age + marital,data=dep)))),2)
```


* For every year older, a persons income decreases by 1%. (`exp(-0.009)` = `r cf[2,1]`)
* Married individuals have a 52% higher income compared to those who are divorced. (`exp(-0.417)` = `r cf[3,1]`)
* Those who have never been married have  16% lower income compared to those who are divorced. (`exp(-0.183)` = `r cf[4,1]`)
* Separated individuals have 32% lower income compared to those who are divorced. (`exp(-0.394)` = `r cf[5,1]`)
* Widowed individuals have 24% lower income compared to those who are divorced. (`exp(-0.278)` = `r cf[6,1]`)

Other references on how to interpret regression parameters when they have 
been log transformed: 

* http://www.ats.ucla.edu/stat/mult_pkg/faq/general/log_transformed_regression.htm 
* http://www.kenbenoit.net/courses/ME104/logmodels2.pdf


   
   